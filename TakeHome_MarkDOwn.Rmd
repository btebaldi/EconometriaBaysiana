---
title: "Econometria Baysiana - Take Home Exam"
Auhtor: "B"
date: "13/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Consider the regression model
$y_i = x_i'\beta + \epsilon_i0$
$\epsilon_i | \lambda_i ,\sigma^2 \sim N(0, \lambda_i\sigma^2)$
where $x_i = (1, x_{i1}, \dots, x_{iq})'$ is a p-dimensional vector of regressors (constant plus q attributes or
characteristics) and the following hierarchical prior for the scale-mixing variables $\lambda_i$:
$\lambda_1, \dots, \lambda_n$ iid Exponential(1/2):

### PART A)

$$\begin{aligned}
p\left(\epsilon_{i} | \sigma^{2}\right) &=\int_{0}^{\infty} p\left(\epsilon_{i} | \lambda_{i}, \sigma^{2}\right) p\left(\lambda_{i}\right) d \lambda_{i} \\
&=\int_{0}^{\infty}\left(2 \pi \lambda_{i} \sigma^{2}\right)^{-1 / 2} \exp \left[-\epsilon_{i}^{2} /\left(2 \lambda_{i} \sigma^{2}\right)\right](1 / 2) \exp \left(-\lambda_{i} / 2\right) d \lambda_{i} \\
&=(1 / 2)\left(2 \pi \sigma^{2}\right)^{-1 / 2} \int_{0}^{\infty} \lambda_{i}^{-1 / 2} \exp \left[-(1 / 2)\left(\lambda_{i}+\left[\epsilon_{i} / \sigma\right]^{2} \lambda_{i}^{-1}\right)\right] d \lambda_{i}
\end{aligned}$$

Now, make a change of variable and let $\psi_i = \lambda_i^{1/2}$. We can then express this integral as

$$
p\left(\epsilon_{i} | \sigma^{2}\right)=\left(2 \pi \sigma^{2}\right)^{-1 / 2} \int_{0}^{\infty} \exp \left(-[1 / 2]\left(\psi_{i}^{2}+\left[\epsilon_{i} / \sigma\right]^{2} \psi_{i}^{-2}\right)\right) d \psi_{i}
$$

The integral in above can be evaluated analytically. Using the following result from Andrews and Mallows (1974) 
$$
\int_{0}^{\infty} \exp \left\{-0.5\left(a^{2} u^{2}+b^{2} u^{-2}\right)\right\} d u=\left(\frac{\pi}{2 a^{2}}\right)^{1/2} \exp \{-|a b|\}
$$
Then $a=1$, $b=\epsilon_i/\sigma$, and $u = \psi_i$

$$
p\left(\epsilon_{i} | \sigma^{2}\right)=\left(2 \pi \sigma^{2}\right)^{-1 / 2} \left(\frac{\pi}{2}\right)^{1/2} \exp\{-|\epsilon_i/\sigma|\} =\frac{1}{2\sigma} \exp\left\{-\frac{|\epsilon_i|}{\sigma}\right\}
$$

###  PART B)

Let $y = (y_1, \dots, y_n)'$ and $X = (x_1, \dots, x_n)'$. Where we have the the following independent priors for $\beta \sim N(\beta_0, V_0)$ and $\sigma^2 \sim IG\left(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2}\right)$. Also let $\mathcal{D} = \{y;X\}$, and $\Lambda$ detnoted as follows
$$
\Lambda  = \begin{bmatrix}\lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ 0 & \vdots & \ddots & 0 \\ 0 & 0 & 0 & \lambda_n \end{bmatrix}
$$
We know that traditional GLS have that $\beta = (X^T\Lambda^{-1}X)^{-1}X^T\Lambda^{-1}y$. If $\beta \sim N(\beta_0, V_0)$, then
$$
\begin{align}
p\left(y | X, \beta, \sigma^{2}, \{\lambda_i\}\right) 
&\propto \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n} \hat\epsilon_i^2 \right\} \\
&\propto \exp \left\{-\frac{1}{2 \sigma^{2}}\left[y^{T} y-2 \beta^{T} X^{T}\Lambda^{-1} y+\beta^{T} X^{T}\Lambda^{-1} X \beta\right]\right\} \\
p\left(\beta | \{\lambda_i\}, \sigma^2, \mathcal{D}\right) &\propto p\left(y | X, \beta, \sigma^{2}\right) \times p(\beta) \\
&\propto \exp \left\{-\frac{1}{2\sigma^2}\left(-2 \beta^{T} X^{T}\Lambda^{-1} y +\beta^{T} X^{T}\Lambda^{-1} X \beta \right)-\frac{1}{2}\left(-2 \beta^{T} V_{0}^{-1} \beta_{0}+\beta^{T} V_{0}^{-1} \beta\right)\right\} \\
&\propto 
\exp \left\{-\frac{1}{2\sigma^2}\left(-2 \beta^{T} (X^{T}\Lambda^{-1} y +V_{0}^{-1} \beta_{0})+ \beta^{T} (X^{T}\Lambda^{-1} X +V_{0}^{-1} ) \beta \right)
\right\}
\end{align}
$$
we recognize this as being proportional to a multivariate normal density, with
$$
\begin{align}
\beta | \{\lambda_i\}, \sigma^2, \mathcal{D} \sim N(\beta_1, V_1) \\
\beta_{1}=V_{1}\left(X^{\prime} \Lambda^{-1} y+V_{0}^{-1} \beta_{0}\right) \qquad V_{1}=\left(X^{\prime} \Lambda^{-1} X / \sigma^{2}+V_{0}^{-1}\right)^{-1}
\end{align}
$$

$$
V_{1}=\left(X^{\prime} \Lambda^{-1} X / \sigma^{2}+V_{0}^{-1}\right)^{-1} \qquad \beta_{1}=V_{1}\left(X^{\prime} \Lambda^{-1} y+V_{0}^{-1} \beta_{0}\right)
$$

As in most normal sampling problems, the semiconjugate prior distribution for $\sigma^2$ is an inverse-gamma distribution.
Letting $ \gamma = 1/\sigma^2$ be the measurement precision, this implies that $\gamma \sim G\left(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2}\right)$ then 
$$
\begin{align}
p(\gamma | \mathcal{D}, \beta) & \propto p(\gamma) p(y | X, \beta, \gamma) \\
& \propto\left[\gamma^{\nu_{0} / 2-1} \exp \left\{-\gamma \times \frac{ \nu_{0} \sigma_{0}^{2}}{2}\right\}\right] \times\left[\gamma^{\frac{n}{2}} \exp \left\{- \frac{\gamma  }{2} \sum_{i=1}^{n} \hat\epsilon_i^2\right\}\right] \\
& \propto\gamma^{\left(\nu_{0}+n\right) / 2-1} \exp \left\{-\gamma\left[\nu_{0} \sigma_{0}^{2}+\sum_{i=1}^{n} \hat\epsilon_i^2\right] / 2\right\}
\end{align}
$$
which we recognize as a gamma density, so that 
$$
\sigma^2| \Lambda, \mathcal{D}, \beta \sim IG\left(\frac{\nu_0 + n}{2}, \frac{\nu_0 \sigma_0^2 + \sum_{i=1}^{n} \hat\epsilon_i^2}{2}\right)
$$
Recall that $\sum_{i=1}^{n} \hat\epsilon_i^2 = (y - X\beta)^T\Lambda^{-1}(y-x\beta)$
$$
\begin{align}
\sigma^2| \Lambda, \mathcal{D}, \beta &\sim IG\left(\frac{\nu_0 + n}{2}, \frac{\nu_0 \sigma_0^2 + (y - X\beta)^T\Lambda^{-1}(y-x\beta)}{2}\right) \\
\sigma^2| \Lambda, \mathcal{D}, \beta &\sim IG\left(\frac{\nu_1}{2}, \frac{\nu_1 \sigma_1^2}{2}\right) \\
&\nu_1 = \nu_0 + n \qquad \nu_1\sigma_1^2 = \nu_0\sigma_0^2 + (y - X\beta)^T\Lambda^{-1}(y-x\beta)
\end{align}
$$
Lastly we have that
$$
\begin{align}
p\left(\lambda_{i} | \beta, \sigma^{2}, y_{i}, x_{i}\right) \propto p\left( y_{i}| \lambda_{i}, \beta, \sigma^{2}, x_{i}\right) p(\lambda_{i}) \\
p\left(\lambda_{i} | \beta, \sigma^{2}, y_{i}, x_{i}\right) \propto \frac{1}{\sqrt{2\pi \sigma^2 \lambda} }  
\exp \left\{-\frac{1}{2}\left(\left(y_{i}-x_{i}^{T} \beta\right)^{2} \sigma^{-2}\lambda_{i}^{-1}\right)\right\} \exp \left\{-0.5 \lambda_{i}\right\} \\
p\left(\lambda_{i} | \beta, \sigma^{2}, y_{i}, x_{i}\right) \propto \lambda^{-1 / 2} \exp \left\{-0.5 \lambda_{i}\right\} 
\exp \left\{-0.5\left(\left(\frac{y_{i}-x_{i}^{\prime} \beta}{\sigma}\right)^{2} \lambda_{i}^{-1}\right)\right\} \\
p\left(\lambda_{i} | \beta, \sigma^{2}, y_{i}, x_{i}\right) \propto \lambda^{-1 / 2} \exp \left\{-0.5\left(\lambda_{i}+\left(\frac{y_{i}-x_{i}^{\prime} \beta}{\sigma}\right)^{2} \lambda_{i}^{-1}\right)\right\}
\end{align}
$$

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
